@inproceedings{wu2018group,
  title={Group normalization},
  author={Wu, Yuxin and He, Kaiming},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}
@article{doi:10.1148/radiol.2392050413,
author = {Rao, Qasim Ali and Newhouse, Jeffrey H.},
title = {Risk of Nephropathy after Intravenous Administration of Contrast Material: A Critical Literature Analysis},
journal = {Radiology},
volume = {239},
number = {2},
pages = {392-397},
year = {2006},
doi = {10.1148/radiol.2392050413},
note ={PMID: 16543592},
URL = { https://doi.org/10.1148/radiol.2392050413},
eprint = { https://doi.org/10.1148/radiol.2392050413}
}

@article{graf2023denoising,
  title={Denoising diffusion-based MRI to CT image translation enables automated spinal segmentation},
  author={Graf, Robert and Schmitt, Joachim and Schlaeger, Sarah and M{\"o}ller, Hendrik Kristian and Sideri-Lampretsa, Vasiliki and Sekuboyina, Anjany and Krieg, Sandro Manuel and Wiestler, Benedikt and Menze, Bjoern and Rueckert, Daniel and others},
  journal={European Radiology Experimental},
  volume={7},
  number={1},
  pages={70},
  year={2023},
  publisher={Springer}
}
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}
@article{dohmen2024similarity,
  title={Similarity and quality metrics for MR image-to-image translation},
  author={Dohmen, Melanie and Klemens, Mark A and Baltruschat, Ivo M and Truong, Tuan and Lenga, Matthias},
  journal={arXiv preprint arXiv:2405.08431},
  year={2024}
}

@ARTICLE{1284395,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}}

@book{euler1769institutionum,
  title={Institutionum calculi integralis},
  author={Euler, L.},
  year={1769},
  publisher = {Impensis Academiae Imperialis Scientiarum},
  url={https://books.google.ru/books?id=XmsCrgEACAAJ},
}

@article{Runge1895,
author = {Runge, C.},
journal = {Mathematische Annalen},
pages = {167-178},
title = {Ueber die numerische Auflösung von Differentialgleichungen.},
url = {http://eudml.org/doc/157756},
volume = {46},
year = {1895},
}
@book{kutta1901beitrag,
  title={Beitrag zur n{\"a}herungsweisen Integration totaler Differentialgleichungen},
  author={Kutta, W.},
  url={https://books.google.ru/books?id=K5e6kQEACAAJ},
  year={1901},
  publisher={Teubner}
}
@book{bashforth1883,
  author    = {Francis Bashforth and John Couch Adams},
  title     = {An Attempt to Test the Theories of Capillary Action by Comparing the Theoretical and Measured Forms of Drops of Fluid},
  year      = {1883},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  url       = {https://archive.org/details/attempttest00bashrich}
}
@article{10.1145/320831.320840,
author = {Lotkin, Mark},
title = {A Note on the Midpoint Method of Integration},
year = {1956},
issue_date = {July 1956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {0004-5411},
url = {https://doi.org/10.1145/320831.320840},
doi = {10.1145/320831.320840},
journal = {J. ACM},
month = jul,
pages = {208–211},
numpages = {4}
}
@InProceedings{10.1007/978-3-319-19992-4_2,
author="Cardoso, M. Jorge
and Sudre, Carole H.
and Modat, Marc
and Ourselin, Sebastien",
editor="Ourselin, Sebastien
and Alexander, Daniel C.
and Westin, Carl-Fredrik
and Cardoso, M. Jorge",
title="Template-Based Multimodal Joint Generative Model of Brain Data",
booktitle="Information Processing in Medical Imaging",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="17--29",
abstract="The advent of large of multi-modal imaging databases opens up the opportunity to learn how local intensity patterns covariate between multiple modalities. These models can then be used to describe expected intensities in an unseen image modalities given one or multiple observations, or to detect deviations (e.g. pathology) from the expected intensity patterns. In this work, we propose a template-based multi-modal generative mixture-model of imaging data and apply it to the problems of inlier/outlier pattern classification and image synthesis. Results on synthetic and patient data demonstrate that the proposed method is able to synthesise unseen data and accurately localise pathological regions, even in the presence of large abnormalities. It also demonstrates that the proposed model can provide accurate and uncertainty-aware intensity estimates of expected imaging patterns.",
isbn="978-3-319-19992-4"
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{yang-etal-2019-convolutional,
    title = "Convolutional Self-Attention Networks",
    author = "Yang, Baosong  and
      Wang, Longyue  and
      Wong, Derek F.  and
      Chao, Lidia S.  and
      Tu, Zhaopeng",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1407/",
    doi = "10.18653/v1/N19-1407",
    pages = "4040--4045",
    abstract = "Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters."
}
@inproceedings{guo2025maisi,
  title={Maisi: Medical ai for synthetic imaging},
  author={Guo, Pengfei and Zhao, Can and Yang, Dong and Xu, Ziyue and Nath, Vishwesh and Tang, Yucheng and Simon, Benjamin and Belue, Mason and Harmon, Stephanie and Turkbey, Baris and others},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  pages={4430--4441},
  year={2025},
  organization={IEEE}
}

@article{armato2011lung,
  title={The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans},
  author={Armato III, Samuel G and McLennan, Geoffrey and Bidaut, Luc and McNitt-Gray, Michael F and Meyer, Charles R and Reeves, Anthony P and Zhao, Binsheng and Aberle, Denise R and Henschke, Claudia I and Hoffman, Eric A and others},
  journal={Medical physics},
  volume={38},
  number={2},
  pages={915--931},
  year={2011},
  publisher={Wiley Online Library}
}

@article{yazdani2025flow,
  title={Flow Matching for Medical Image Synthesis: Bridging the Gap Between Speed and Quality},
  author={Yazdani, Milad and Medghalchi, Yasamin and Ashrafian, Pooria and Hacihaliloglu, Ilker and Shahriari, Dena},
  journal={arXiv preprint arXiv:2503.00266},
  year={2025}
}

@inproceedings{kim2024adaptive,
  title={Adaptive latent diffusion model for 3d medical image to image translation: Multi-modal magnetic resonance imaging study},
  author={Kim, Jonghun and Park, Hyunjin},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={7604--7613},
  year={2024}
}

@article{article_nature,
author = {Kalantar, Reza and Hindocha, Sumeet and Hunter, Benjamin and Sharma, Bhupinder and Khan, Nasir and Koh, Dow-Mu and Ahmed, Merina and Aboagye, Eric and Lee, Richard and Blackledge, Matthew},
year = {2023},
month = {06},
pages = {},
title = {Non-contrast CT synthesis using patch-based cycle-consistent generative adversarial network (Cycle-GAN) for radiomics and deep learning in the era of COVID-19},
volume = {13},
journal = {Scientific Reports},
doi = {10.1038/s41598-023-36712-1}
}

@article{article_dual,
author = {Kim, Jungye and Lee, Jimin and Kim, Bitbyeol and Kim, Sangwook and Jin, Hyeongmin and Jung, Seongmoon},
year = {2024},
month = {12},
pages = {},
title = {Generation of deep learning based virtual non-contrast CT using dual-layer dual-energy CT and its application to planning CT for radiotherapy},
volume = {19},
journal = {PLOS ONE},
doi = {10.1371/journal.pone.0316099}
}

@article{reynaud2025echoflow,
  title={EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation},
  author={Reynaud, Hadrien and Gomez, Alberto and Leeson, Paul and Meng, Qingjie and Kainz, Bernhard},
  journal={arXiv preprint arXiv:2503.22357},
  year={2025}
}

@inproceedings{hu2022domain,
  title={Domain-adaptive 3d medical image synthesis: An efficient unsupervised approach},
  author={Hu, Qingqiao and Li, Hongwei and Zhang, Jianguo},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={495--504},
  year={2022},
  organization={Springer}
}

@inproceedings{cao2022swin,
  title={Swin-unet: Unet-like pure transformer for medical image segmentation},
  author={Cao, Hu and Wang, Yueyue and Chen, Joy and Jiang, Dongsheng and Zhang, Xiaopeng and Tian, Qi and Wang, Manning},
  booktitle={European conference on computer vision},
  pages={205--218},
  year={2022},
  organization={Springer}
}

@article{kalantar2023non,
  title={Non-contrast CT synthesis using patch-based cycle-consistent generative adversarial network (Cycle-GAN) for radiomics and deep learning in the era of COVID-19},
  author={Kalantar, Reza and Hindocha, Sumeet and Hunter, Benjamin and Sharma, Bhupinder and Khan, Nasir and Koh, Dow-Mu and Ahmed, Merina and Aboagye, Eric O and Lee, Richard W and Blackledge, Matthew D},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={10568},
  year={2023},
  publisher={Nature Publishing Group},
  doi={10.1038/s41598-023-36712-1}
}

@article{hu2021bidirectional,
  title={Bidirectional mapping generative adversarial networks for brain MR to PET synthesis},
  author={Hu, Shengye and Lei, Baiying and Wang, Shuqiang and Wang, Yong and Feng, Zhiguang and Shen, Yanyan},
  journal={IEEE Transactions on Medical Imaging},
  volume={41},
  number={1},
  pages={145--157},
  year={2021},
  publisher={IEEE}
}

@article{WEI2019101546,
title = {Predicting PET-derived demyelination from multimodal MRI using sketcher-refiner adversarial training for multiple sclerosis},
journal = {Medical Image Analysis},
volume = {58},
pages = {101546},
year = {2019},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2019.101546},
url = {https://www.sciencedirect.com/science/article/pii/S1361841519300817},
author = {Wen Wei and Emilie Poirion and Benedetta Bodini and Stanley Durrleman and Nicholas Ayache and Bruno Stankoff and Olivier Colliot},
keywords = {Multimodal MRI, PET Imaging, Adversarial training, Multiple sclerosis},
abstract = {Multiple sclerosis (MS) is the most common demyelinating disease. In MS, demyelination occurs in the white matter of the brain and in the spinal cord. It is thus essential to measure the tissue myelin content to understand the physiopathology of MS, track progression and assess treatment efficacy. Positron emission tomography (PET) with [11C]PIB is a reliable method to measure myelin content in vivo. However, the availability of PET in clinical centers is limited. Moreover, it is expensive to acquire and invasive due to the injection of a radioactive tracer. By contrast, MR imaging is non-invasive, less expensive and widely available, but conventional MRI sequences cannot provide a direct and reliable measure of myelin. In this work, we therefore propose, to the best of our knowledge for the first time, a method to predict the PET-derived myelin content map from multimodal MRI. To that purpose, we introduce a new approach called Sketcher-Refiner generative adversarial networks (GANs) with specifically designed adversarial loss functions. The first network (Sketcher) generates global anatomical and physiological information. The second network (Refiner) refines and generates the tissue myelin content. A visual attention saliency map is also proposed to interpret the attention of neural networks. Our approach is shown to outperform the state-of-the-art methods in terms of image quality and myelin content prediction. Particularly, our prediction results show similar results to the PET-derived gold standard at both global and voxel-wise levels indicating the potential for clinical management of patients with MS.}
}

@article{https://doi.org/10.1002/mrm.28819,
author = {Lan, Haoyu and the Alzheimer Disease Neuroimaging Initiative and Toga, Arthur W. and Sepehrband, Farshid},
title = {Three-dimensional self-attention conditional GAN with spectral normalization for multimodal neuroimaging synthesis},
journal = {Magnetic Resonance in Medicine},
volume = {86},
number = {3},
pages = {1718-1733},
keywords = {3D GAN, MRI, PET, self-attention, spectral normalization, synthesis},
doi = {https://doi.org/10.1002/mrm.28819},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.28819},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.28819},
abstract = {Purpose To develop a new 3D generative adversarial network that is designed and optimized for the application of multimodal 3D neuroimaging synthesis. Methods We present a 3D conditional generative adversarial network (GAN) that uses spectral normalization and feature matching to stabilize the training process and ensure optimization convergence (called SC-GAN). A self-attention module was also added to model the relationships between widely separated image voxels. The performance of the network was evaluated on the data set from ADNI-3, in which the proposed network was used to predict PET images, fractional anisotropy, and mean diffusivity maps from multimodal MRI. Then, SC-GAN was applied on a multidimensional diffusion MRI experiment for superresolution application. Experiment results were evaluated by normalized RMS error, peak SNR, and structural similarity. Results In general, SC-GAN outperformed other state-of-the-art GAN networks including 3D conditional GAN in all three tasks across all evaluation metrics. Prediction error of the SC-GAN was 18\%, 24\% and 29\% lower compared to 2D conditional GAN for fractional anisotropy, PET and mean diffusivity tasks, respectively. The ablation experiment showed that the major contributors to the improved performance of SC-GAN are the adversarial learning and the self-attention module, followed by the spectral normalization module. In the superresolution multidimensional diffusion experiment, SC-GAN provided superior predication in comparison to 3D Unet and 3D conditional GAN. Conclusion In this work, an efficient end-to-end framework for multimodal 3D medical image synthesis (SC-GAN) is presented. The source code is also made available at https://github.com/Haoyulance/SC-GAN.},
year = {2021}
}
@article{Lin2021BidirectionalMO,
  title={Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer’s Disease},
  author={Wanyun Lin and Weiming Lin and Gang Chen and Hejun Zhang and Qinquan Gao and Yechong Huang and Tong Tong and Min Du},
  journal={Frontiers in Neuroscience},
  year={2021},
  volume={15},
  url={https://api.semanticscholar.org/CorpusID:233186986}
}
@article{sikka2021mri,
  title={MRI to PET cross-modality translation using globally and locally aware GAN (GLA-GAN) for multi-modal diagnosis of Alzheimer's disease},
  author={Sikka, Apoorva and Virk, Jitender Singh and Bathula, Deepti R and others},
  journal={arXiv preprint arXiv:2108.02160},
  year={2021}
}
@article{ZHANG2022106676,
title = {BPGAN: Brain PET synthesis from MRI using generative adversarial network for multi-modal Alzheimer’s disease diagnosis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {217},
pages = {106676},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106676},
url = {https://www.sciencedirect.com/science/article/pii/S016926072200061X},
author = {Jin Zhang and Xiaohai He and Linbo Qing and Feng Gao and Bin Wang},
keywords = {Alzheimer’s disease, Medical imaging synthesis, MRI, PET, Generative adversarial networks},
abstract = {Background and Objective
Multi-modal medical images, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), have been widely used for the diagnosis of brain disorder diseases like Alzheimer’s disease (AD) since they can provide various information. PET scans can detect cellular changes in organs and tissues earlier than MRI. Unlike MRI, PET data is difficult to acquire due to cost, radiation, or other limitations. Moreover, PET data is missing for many subjects in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. To solve this problem, a 3D end-to-end generative adversarial network (named BPGAN) is proposed to synthesize brain PET from MRI scans, which can be used as a potential data completion scheme for multi-modal medical image research.
Methods
We propose BPGAN, which learns an end-to-end mapping function to transform the input MRI scans to their underlying PET scans. First, we design a 3D multiple convolution U-Net (MCU) generator architecture to improve the visual quality of synthetic results while preserving the diverse brain structures of different subjects. By further employing a 3D gradient profile (GP) loss and structural similarity index measure (SSIM) loss, the synthetic PET scans have higher-similarity to the ground truth. In this study, we explore alternative data partitioning ways to study their impact on the performance of the proposed method in different medical scenarios.
Results
We conduct experiments on a publicly available ADNI database. The proposed BPGAN is evaluated by mean absolute error (MAE), peak-signal-to-noise-ratio (PSNR) and SSIM, superior to other compared models in these quantitative evaluation metrics. Qualitative evaluations also validate the effectiveness of our approach. Additionally, combined with MRI and our synthetic PET scans, the accuracies of multi-class AD diagnosis on dataset-A and dataset-B are 85.00% and 56.47%, which have been improved by about 1% and 1%, respectively, compared to the stand-alone MRI.
Conclusions
The experimental results of quantitative measures, qualitative displays, and classification evaluation demonstrate that the synthetic PET images by BPGAN are reasonable and high-quality, which provide complementary information to improve the performance of AD diagnosis. This work provides a valuable reference for multi-modal medical image analysis.}
}

@Article{s22124640,
AUTHOR = {Bazangani, Farideh and Richard, Frédéric J. P. and Ghattas, Badih and Guedj, Eric},
TITLE = {FDG-PET to T1 Weighted MRI Translation with 3D Elicit Generative Adversarial Network (E-GAN)},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {12},
ARTICLE-NUMBER = {4640},
URL = {https://www.mdpi.com/1424-8220/22/12/4640},
PubMedID = {35746422},
ISSN = {1424-8220},
ABSTRACT = {Objective: With the strengths of deep learning, computer-aided diagnosis (CAD) is a hot topic for researchers in medical image analysis. One of the main requirements for training a deep learning model is providing enough data for the network. However, in medical images, due to the difficulties of data collection and data privacy, finding an appropriate dataset (balanced, enough samples, etc.) is quite a challenge. Although image synthesis could be beneficial to overcome this issue, synthesizing 3D images is a hard task. The main objective of this paper is to generate 3D T1 weighted MRI corresponding to FDG-PET. In this study, we propose a separable convolution-based Elicit generative adversarial network (E-GAN). The proposed architecture can reconstruct 3D T1 weighted MRI from 2D high-level features and geometrical information retrieved from a Sobel filter. Experimental results on the ADNI datasets for healthy subjects show that the proposed model improves the quality of images compared with the state of the art. In addition, the evaluation of E-GAN and the state of art methods gives a better result on the structural information (13.73% improvement for PSNR and 22.95% for SSIM compared to Pix2Pix GAN) and textural information (6.9% improvements for homogeneity error in Haralick features compared to Pix2Pix GAN).},
DOI = {10.3390/s22124640}
}

@article{bilovs2021neural,
  title={Neural flows: Efficient alternative to neural ODEs},
  author={Bilo{\v{s}}, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and G{\"u}nnemann, Stephan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={21325--21337},
  year={2021}
}

@article{WANG2024102983,
title = {3D multi-modality Transformer-GAN for high-quality PET reconstruction},
journal = {Medical Image Analysis},
volume = {91},
pages = {102983},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.102983},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002438},
author = {Yan Wang and Yanmei Luo and Chen Zu and Bo Zhan and Zhengyang Jiao and Xi Wu and Jiliu Zhou and Dinggang Shen and Luping Zhou},
keywords = {Positron emission tomography (PET), Transformer, Multi-modality, Generative adversarial network (GAN), PET reconstruction},
abstract = {Positron emission tomography (PET) scans can reveal abnormal metabolic activities of cells and provide favorable information for clinical patient diagnosis. Generally, standard-dose PET (SPET) images contain more diagnostic information than low-dose PET (LPET) images but higher-dose scans can also bring higher potential radiation risks. To reduce the radiation risk while acquiring high-quality PET images, in this paper, we propose a 3D multi-modality edge-aware Transformer-GAN for high-quality SPET reconstruction using the corresponding LPET images and T1 acquisitions from magnetic resonance imaging (T1-MRI). Specifically, to fully excavate the metabolic distributions in LPET and anatomical structural information in T1-MRI, we first use two separate CNN-based encoders to extract local spatial features from the two modalities, respectively, and design a multimodal feature integration module to effectively integrate the two kinds of features given the diverse contributions of features at different locations. Then, as CNNs can describe local spatial information well but have difficulty in modeling long-range dependencies in images, we further apply a Transformer-based encoder to extract global semantic information in the input images and use a CNN decoder to transform the encoded features into SPET images. Finally, a patch-based discriminator is applied to ensure the similarity of patch-wise data distribution between the reconstructed and real images. Considering the importance of edge information in anatomical structures for clinical disease diagnosis, besides voxel-level estimation error and adversarial loss, we also introduce an edge-aware loss to retain more edge detail information in the reconstructed SPET images. Experiments on the phantom dataset and clinical dataset validate that our proposed method can effectively reconstruct high-quality SPET images and outperform current state-of-the-art methods in terms of qualitative and quantitative metrics.}
}

@article{durrer2023diffusion,
  title={Diffusion models for contrast harmonization of magnetic resonance images},
  author={Durrer, Alicia and Wolleb, Julia and Bieder, Florentin and Sinnecker, Tim and Weigel, Matthias and Sandk{\"u}hler, Robin and Granziera, Cristina and Yaldizli, {\"O}zg{\"u}r and Cattin, Philippe C},
  journal={arXiv preprint arXiv:2303.08189},
  year={2023}
}

@inproceedings{pan2025cycle,
  title={Cycle-guided denoising diffusion probability model for 3d cross-modality mri synthesis},
  author={Pan, Shaoyan and Eidex, Zach and Safari, Mojtaba and Qiu, Richard and Yang, Xiaofeng},
  booktitle={Medical Imaging 2025: Clinical and Biomedical Imaging},
  volume={13410},
  pages={515--522},
  year={2025},
  organization={SPIE}
}

@inproceedings{zhu2023make,
  title={Make-a-volume: Leveraging latent diffusion models for cross-modality 3d brain mri synthesis},
  author={Zhu, Lingting and Xue, Zeyue and Jin, Zhenchao and Liu, Xian and He, Jingzhen and Liu, Ziwei and Yu, Lequan},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={592--601},
  year={2023},
  organization={Springer}
}

@article{zhu2024generative,
  title={Generative enhancement for 3d medical images},
  author={Zhu, Lingting and Codella, Noel and Chen, Dongdong and Jin, Zhenchao and Yuan, Lu and Yu, Lequan},
  journal={arXiv preprint arXiv:2403.12852},
  year={2024}
}

@article{dorjsembe2024conditional,
  title={Conditional diffusion models for semantic 3d brain mri synthesis},
  author={Dorjsembe, Zolnamar and Pao, Hsing-Kuo and Odonchimed, Sodtavilan and Xiao, Furen},
  journal={IEEE Journal of Biomedical and Health Informatics},
  year={2024},
  publisher={IEEE}
}

@article{pan2024synthetic,
  title={Synthetic CT generation from MRI using 3D transformer-based denoising diffusion model},
  author={Pan, Shaoyan and Abouei, Elham and Wynne, Jacob and Chang, Chih-Wei and Wang, Tonghe and Qiu, Richard LJ and Li, Yuheng and Peng, Junbo and Roper, Justin and Patel, Pretesh and others},
  journal={Medical Physics},
  volume={51},
  number={4},
  pages={2538--2548},
  year={2024},
  publisher={Wiley Online Library}
}

@inproceedings{li2024pasta,
  title={PASTA: P athology-A ware MRI to PET Cro S s-modal T r A nslation with Diffusion Models},
  author={Li, Yitong and Yakushev, Igor and Hedderich, Dennis M and Wachinger, Christian},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={529--540},
  year={2024},
  organization={Springer}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

@article{albergo2022building,
  title={Building normalizing flows with stochastic interpolants},
  author={Albergo, Michael S and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:2209.15571},
  year={2022}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@article{tong2023improving,
  title={Improving and generalizing flow-based generative models with minibatch optimal transport},
  author={Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and Rector-Brooks, Jarrid and Wolf, Guy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2302.00482},
  year={2023}
}

@article{li2024abdomenatlas,
  title={Abdomenatlas: A large-scale, detailed-annotated, \& multi-center dataset for efficient transfer learning and open algorithmic benchmarking},
  author={Li, Wenxuan and Qu, Chongyu and Chen, Xiaoxi and Bassi, Pedro RAS and Shi, Yijia and Lai, Yuxiang and Yu, Qian and Xue, Huimin and Chen, Yixiong and Lin, Xiaorui and others},
  journal={Medical Image Analysis},
  volume={97},
  pages={103285},
  year={2024},
  publisher={Elsevier}
}

@article{mckinney2020international,
  title={International evaluation of an AI system for breast cancer screening},
  author={McKinney, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg S and Darzi, Ara and others},
  journal={Nature},
  volume={577},
  number={7788},
  pages={89--94},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{ardila2019end,
  title={End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography},
  author={Ardila, Diego and Kiraly, Atilla P and Bharadwaj, Sujeeth and Choi, Bokyung and Reicher, Joshua J and Peng, Lily and Tse, Daniel and Etemadi, Mozziyar and Ye, Wenxing and Corrado, Greg and others},
  journal={Nature medicine},
  volume={25},
  number={6},
  pages={954--961},
  year={2019},
  publisher={Nature Publishing Group US New York}
}

@article{esteva2017dermatologist,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={nature},
  volume={542},
  number={7639},
  pages={115--118},
  year={2017},
  publisher={Nature Publishing Group UK London}
}

@article{gulshan2016development,
  title={Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs},
  author={Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and others},
  journal={jama},
  volume={316},
  number={22},
  pages={2402--2410},
  year={2016},
  publisher={American Medical Association}
}

@article{topol2019high,
  title={High-performance medicine: the convergence of human and artificial intelligence},
  author={Topol, Eric J},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={44--56},
  year={2019},
  publisher={Nature Publishing Group US New York}
}

@article{dembrower2023artificial,
  title={Artificial intelligence for breast cancer detection in screening mammography in Sweden: a prospective, population-based, paired-reader, non-inferiority study},
  author={Dembrower, Karin and Crippa, Alessio and Col{\'o}n, Eugenia and Eklund, Martin and Strand, Fredrik},
  journal={The Lancet Digital Health},
  volume={5},
  number={10},
  pages={e703--e711},
  year={2023},
  publisher={Elsevier}
}

@article{cardoso2022monai,
  title={Monai: An open-source framework for deep learning in healthcare},
  author={Cardoso, M Jorge and Li, Wenqi and Brown, Richard and Ma, Nic and Kerfoot, Eric and Wang, Yiheng and Murrey, Benjamin and Myronenko, Andriy and Zhao, Can and Yang, Dong and others},
  journal={arXiv preprint arXiv:2211.02701},
  year={2022}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, A},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}

@inproceedings{korhonen2012peak,
  title={Peak signal-to-noise ratio revisited: Is simple beautiful?},
  author={Korhonen, Jari and You, Junyong},
  booktitle={2012 Fourth international workshop on quality of multimedia experience},
  pages={37--38},
  year={2012},
  organization={IEEE}
}

@ARTICLE{927467,
  author={Cootes, T.F. and Edwards, G.J. and Taylor, C.J.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Active appearance models}, 
  year={2001},
  volume={23},
  number={6},
  pages={681-685},
  keywords={Deformable models;Shape control;Iterative algorithms;Optimization methods;Active shape model;Robustness;Image reconstruction;Surface fitting;Image generation;Image segmentation},
  doi={10.1109/34.927467}
}

@article{aubert2006twenty,
  title={Twenty new digital brain phantoms for creation of validation image data bases},
  author={Aubert-Broche, Bereng{\`e}re and Griffin, Mark and Pike, G Bruce and Evans, Alan C and Collins, D Louis},
  journal={IEEE transactions on medical imaging},
  volume={25},
  number={11},
  pages={1410--1416},
  year={2006},
  publisher={IEEE}
}

@article{PRASTAWA2009297,
title = {Simulation of brain tumors in MR images for evaluation of segmentation efficacy},
journal = {Medical Image Analysis},
volume = {13},
number = {2},
pages = {297-311},
year = {2009},
note = {Includes Special Section on Functional Imaging and Modelling of the Heart},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2008.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1361841508001357},
author = {Marcel Prastawa and Elizabeth Bullitt and Guido Gerig},
keywords = {Brain MRI, Segmentation validation, Tumor simulation, Simulation of tumor infiltration, Diffusion tensor imaging, Ground truth, Gold standard},
abstract = {Obtaining validation data and comparison metrics for segmentation of magnetic resonance images (MRI) are difficult tasks due to the lack of reliable ground truth. This problem is even more evident for images presenting pathology, which can both alter tissue appearance through infiltration and cause geometric distortions. Systems for generating synthetic images with user-defined degradation by noise and intensity inhomogeneity offer the possibility for testing and comparison of segmentation methods. Such systems do not yet offer simulation of sufficiently realistic looking pathology. This paper presents a system that combines physical and statistical modeling to generate synthetic multi-modal 3D brain MRI with tumor and edema, along with the underlying anatomical ground truth, Main emphasis is placed on simulation of the major effects known for tumor MRI, such as contrast enhancement, local distortion of healthy tissue, infiltrating edema adjacent to tumors, destruction and deformation of fiber tracts, and multi-modal MRI contrast of healthy tissue and pathology. The new method synthesizes pathology in multi-modal MRI and diffusion tensor imaging (DTI) by simulating mass effect, warping and destruction of white matter fibers, and infiltration of brain tissues by tumor cells. We generate synthetic contrast enhanced MR images by simulating the accumulation of contrast agent within the brain. The appearance of the the brain tissue and tumor in MRI is simulated by synthesizing texture images from real MR images. The proposed method is able to generate synthetic ground truth and synthesized MR images with tumor and edema that exhibit comparable segmentation challenges to real tumor MRI. Such image data sets will find use in segmentation reliability studies, comparison and validation of different segmentation methods, training and teaching, or even in evaluating standards for tumor size like the RECIST criteria (response evaluation criteria in solid tumors).}
}

@article{segars20104d,
  title={4D XCAT phantom for multimodality imaging research},
  author={Segars, W Paul and Sturgeon, G and Mendonca, S and Grimes, Jason and Tsui, Benjamin MW},
  journal={Medical physics},
  volume={37},
  number={9},
  pages={4902--4915},
  year={2010},
  publisher={Wiley Online Library}
}

@article{jan2004gate,
  title={GATE: a simulation toolkit for PET and SPECT},
  author={Jan, S{\'e}bastien and Santin, Giovanni and Strul, Daniel and Staelens, Steven and Assi{\'e}, K and Autret, Damien and Avner, St{\'e}phane and Barbier, Remi and Bardi{\`e}s, Manuel and Bloomfield, PM and others},
  journal={Physics in Medicine \& Biology},
  volume={49},
  number={19},
  pages={4543},
  year={2004},
  publisher={IOP Publishing}
}

@inproceedings{peng2022towards,
  title={Towards performant and reliable undersampled MR reconstruction via diffusion model sampling},
  author={Peng, Cheng and Guo, Pengfei and Zhou, S Kevin and Patel, Vishal M and Chellappa, Rama},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={623--633},
  year={2022},
  organization={Springer}
}

@inproceedings{xie2022measurement,
  title={Measurement-conditioned denoising diffusion probabilistic model for under-sampled medical image reconstruction},
  author={Xie, Yutong and Li, Quanzheng},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={655--664},
  year={2022},
  organization={Springer}
}

@article{pham2019multiscale,
  title={Multiscale brain MRI super-resolution using deep 3D convolutional networks},
  author={Pham, Chi-Hieu and Tor-D{\'\i}ez, Carlos and Meunier, H{\'e}l{\`e}ne and Bednarek, Nathalie and Fablet, Ronan and Passat, Nicolas and Rousseau, Fran{\c{c}}ois},
  journal={Computerized Medical Imaging and Graphics},
  volume={77},
  pages={101647},
  year={2019},
  publisher={Elsevier}
}

@article{ensemblect,
  title={CT Super-resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble},
  author={Ensemble, Cycle Learning}
}
@article{10.1001/jamainternmed.2025.0505,
    author = {Smith-Bindman, Rebecca and Chu, Philip W. and Azman Firdaus, Hana and Stewart, Carly and Malekhedayat, Matthew and Alber, Susan and Bolch, Wesley E. and Mahendra, Malini and Berrington de González, Amy and Miglioretti, Diana L.},
    title = {Projected Lifetime Cancer Risks From Current Computed Tomography Imaging},
    journal = {JAMA Internal Medicine},
    year = {2025},
    month = {04},
    abstract = {Approximately 93 million computed tomography (CT) examinations are performed on 62 million patients annually in the United States, and ionizing radiation from CT is a known carcinogen.To project the number of future lifetime cancers in the US population associated with CT imaging in 2023.This risk model used a multicenter sample of CT examinations prospectively assembled between January 2018 and December 2020 from the University of California San Francisco International CT Dose Registry. Data analysis was conducted from October 2023 to October 2024.Distributions of CT examinations and associated organ-specific radiation doses were estimated by patient age, sex, and CT category and scaled to the US population based on the number of examinations in 2023, quantified by the IMV national survey. Lifetime radiation-induced cancer incidence and 90\% uncertainty limits (UL) were estimated by age, sex, and CT category using National Cancer Institute software based on the National Research Council’s Biological Effects of Ionizing Radiation VII models and projected to the US population using scaled examination counts.An estimated 61 510 000 patients underwent 93 000 000 CT examinations in 2023, including 2 570 000 (4.2\%) children, 58 940 000 (95.8\%) adults, 32 600 000 (53.0\%) female patients, and 28 910 000 (47.0\%) male patients. Approximately 103 000 (90\% UL, 96 400-109 500) radiation-induced cancers were projected to result from these examinations. Estimated radiation-induced cancer risks were higher in children and adolescents, yet higher CT utilization in adults accounted for most (93 000; 90\% UL, 86 900-99 600 [91\%]) radiation-induced cancers. The most common cancers were lung cancer (22 400 cases; 90\% UL, 20 200-25 000 cases), colon cancer (8700 cases; 90\% UL, 7800-9700 cases), leukemia (7900 cases; 90\% UL, 6700-9500 cases), and bladder cancer (7100 cases, 90\% UL, 6000-8500 cases) overall, while in female patients, breast was second most common (5700 cases; 90\% UL, 5000-6500 cases). The largest number of cancers was projected to result from abdomen and pelvis CT in adults, reflecting 37 500 of 103 000 cancers (37\%) and 30 million of 93 million CT examinations (32\%), followed by chest CT (21 500 cancers [21\%]; 20 million examinations [21\%]). Estimates remained large over a variety of sensitivity analyses, which resulted in a range of 80 000 to 127 000 projected cancers across analyses.This study found that at current utilization and radiation dose levels, CT examinations in 2023 were projected to result in approximately 103 000 future cancers over the course of the lifetime of exposed patients. If current practices persist, CT-associated cancer could eventually account for 5\% of all new cancer diagnoses annually.},
    issn = {2168-6106},
    doi = {10.1001/jamainternmed.2025.0505},
    url = {https://doi.org/10.1001/jamainternmed.2025.0505},
    eprint = {https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2832778/jamainternal\_smithbindman\_2025\_oi\_250011\_1744156384.58838.pdf},
}

@article{VIRARKAR2022293,
title = {Virtual Non-contrast Imaging in The Abdomen and The Pelvis: An Overview},
journal = {Seminars in Ultrasound, CT and MRI},
volume = {43},
number = {4},
pages = {293-310},
year = {2022},
note = {Dual source CT: Applications, Technology},
issn = {0887-2171},
doi = {https://doi.org/10.1053/j.sult.2022.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0887217122000312},
author = {Mayur K Virarkar and Sai Swarupa R Vulasala and Anjali Verma Gupta and DheerajReddy Gopireddy and Sindhu Kumar and Mauricio Hernandez and Chandana Lall and Priya Bhosale},
abstract = {Virtual non-contrast (VNC) imaging is a post-processing technique generated from contrast-enhanced scans using dual-energy computed tomography (DECT). It is generated by removing iodine from imaging acquired at multiple energies. Myriad clinical studies have shown its ability to diagnose the various abdominal and pelvic pathologies discussed in the article. VNC is also a problem-solving tool for characterizing incidentally detected lesions (“incidentalomas”), often decreasing the need for additional follow-up imaging. It also obviates the multiphase image acquisitions to evaluate hematuria, hepatic steatosis, aortic endoleaks, and gastrointestinal bleeding by generating image datasets from different tissue attenuation values. The scope of this article is to provide an overview of various applications of VNC imaging obtained by DECT in the abdomen and pelvis.}
}